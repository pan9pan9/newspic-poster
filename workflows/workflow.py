from crawlers.newspick_crawler import NewspickCrawler
from apis.threads_api import ThreadsAPI
import asyncio

# workflows/workflow.py

async def run_workflow(crawler, threads_api, limit=20):
    """
    ë‰´ìŠ¤í”½ ê¸°ì‚¬ í¬ë¡¤ë§ í›„ Threadsì— ì—…ë¡œë“œ + ë§í¬ ëŒ“ê¸€ ë‹¬ê¸°
    :param crawler: NewspickCrawler ê°ì²´
    :param threads_api: ThreadsAPI ê°ì²´
    :param limit: ìµœëŒ€ ì²˜ë¦¬í•  ê¸°ì‚¬ ê°œìˆ˜
    """
    print("ğŸ‘‰ ë‰´ìŠ¤í”½ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
    articles = await crawler.fetch_articles()
    print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    thread_ids = []

    # 1ï¸âƒ£ ì´ë¯¸ì§€ + ì œëª© ì—…ë¡œë“œ
    for idx, article in enumerate(articles[:limit]):
        print(f"ğŸ“¤ {idx+1}ë²ˆì§¸ ê¸°ì‚¬ ì—…ë¡œë“œ ì¤‘: {article['title']}")

        # ë¯¸ë””ì–´ ì»¨í…Œì´ë„ˆ ìƒì„±
        media_response = threads_api.create_media(
            media_type="IMAGE",
            image_url=article["img"],
            text=article["title"]
        )
        container_id = media_response.get("id")
        if not container_id:
            print(f"âš ï¸ ë¯¸ë””ì–´ ì»¨í…Œì´ë„ˆ ìƒì„± ì‹¤íŒ¨: {media_response}")
            continue

        # ë¯¸ë””ì–´ ë°œí–‰
        publish_response = threads_api.publish_media(container_id)
        post_id = publish_response.get("id")
        if not post_id:
            print(f"âš ï¸ ê²Œì‹œë¬¼ ë°œí–‰ ì‹¤íŒ¨: {publish_response}")
            continue

        print(f"âœ… {idx+1}ë²ˆì§¸ ê²Œì‹œë¬¼ ì—…ë¡œë“œ ì™„ë£Œ, post_id: {post_id}")
        thread_ids.append(post_id)

    print("ğŸ“ ê¸°ì‚¬ ë§í¬ ëŒ“ê¸€ ë‹¬ê¸° ì‹œì‘")
    # 2ï¸âƒ£ ëŒ“ê¸€ ë‹¬ê¸° (ë§í¬)
    for idx, article in enumerate(articles[:limit]):
        if idx >= len(thread_ids):
            print(f"âš ï¸ ê²Œì‹œë¬¼ì´ ìƒì„±ë˜ì§€ ì•Šì•„ ëŒ“ê¸€ ìƒëµ: {article['title']}")
            continue

        reply_response = threads_api.reply_to_post(thread_ids[idx], article["link"])
        print(f"ğŸ’¬ ëŒ“ê¸€ ë‹¬ê¸° ì‘ë‹µ: {reply_response}")