import asyncio
import logging
from playwright.async_api import async_playwright
from crawlers.locators import LoginPageLocators, ArticlePageLocators

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

class NewspickCrawler:
    def __init__(self, user_id: str, password: str):
        self.user_id = user_id
        self.password = password

    async def fetch_articles(self, limit: int = 20):
        async with async_playwright() as p:
            logger.info("ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰")
            browser = await p.chromium.launch(headless=True)

            context = await browser.new_context(
                permissions=["clipboard-read", "clipboard-write"]
            )
            page = await context.new_page()
            page.on("dialog", lambda dialog: dialog.accept())

            # ë¡œê·¸ì¸
            logger.info("ğŸ”‘ ë¡œê·¸ì¸ ì‹œë„")
            await page.goto("https://partners.newspic.kr/main/index")
            await page.fill(LoginPageLocators.ID_INPUT, self.user_id)
            await page.fill(LoginPageLocators.PASSWORD_INPUT, self.password)
            await page.click(LoginPageLocators.LOGIN_BUTTON)
            await page.wait_for_timeout(3000)
            logger.info(f"âœ… ë¡œê·¸ì¸ ì™„ë£Œ, í˜„ì¬ URL: {page.url}")

            # ì´ë¯¸ì§€ ë¡œë”©
            try:
                await page.wait_for_selector(ArticlePageLocators.IMAGE, timeout=10000)
            except:
                logger.warning("âš ï¸ ì´ë¯¸ì§€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                await browser.close()
                return []

            # ì´ë¯¸ì§€ ëª©ë¡
            img_elements = await page.locator(ArticlePageLocators.IMAGE).all()
            logger.info(f"ğŸ” ì´ë¯¸ì§€ ìš”ì†Œ ê°œìˆ˜: {len(img_elements)}")
            img_src_list = await page.locator(ArticlePageLocators.IMAGE).evaluate_all(
                "imgs => imgs.map(img => img.src)"
            )
            img_src_list = img_src_list[1:limit+1]

            # ì œëª© ëª©ë¡
            title_elements = await page.locator(ArticlePageLocators.TITLE).all_inner_texts()
            logger.info(f"ğŸ” ì œëª© ìš”ì†Œ ê°œìˆ˜: {len(title_elements)}")
            title_list = [
                t.replace(" â€¦", "").replace("'", " ").replace('"', " ")
                for t in title_elements[1:limit+1]
            ]

            # ë²„íŠ¼ ëª©ë¡
            buttons = await page.locator(ArticlePageLocators.COPY_BUTTON).all()
            thumbs = await page.locator(ArticlePageLocators.THUMB).all()
            logger.info(f"ğŸ” ë²„íŠ¼ ìš”ì†Œ ê°œìˆ˜: {len(buttons)}")

            links = []
            for idx, button in enumerate(buttons[:limit]):
                await thumbs[idx].hover()
                await page.mouse.down()
                await button.click()
                await page.wait_for_timeout(1000)
                await page.mouse.up()

                copied_link = await page.evaluate("""
                    (async () => {
                        try {
                            const text = await navigator.clipboard.readText();
                            return text;
                        } catch (err) {
                            console.error('í´ë¦½ë³´ë“œ ì½ê¸° ì‹¤íŒ¨: ', err);
                            return null;
                        }
                    })()
                """)
                links.append(copied_link)
                print(f"ğŸ”— {idx+1}ë²ˆì§¸ ë§í¬ ìˆ˜ì§‘: {copied_link}")
                logger.info(f"ğŸ”— {idx+1}ë²ˆì§¸ ë§í¬ ìˆ˜ì§‘: {copied_link}")

            await browser.close()
            logger.info("ğŸŒ ë¸Œë¼ìš°ì € ì¢…ë£Œ")

        return [
            {"title": t, "img": i, "link": l}
            for t, i, l in zip(title_list, img_src_list, links)
        ]